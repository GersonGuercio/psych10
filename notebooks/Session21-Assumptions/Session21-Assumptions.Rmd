---
title: 'Session 21: Assumptions of modeling'
output:
  html_document:
    df_print: paged
---

In this notebook we will examine the various assumptions made in statistical modeling.

```{r}
library(ggplot2)
library(dplyr)
library(tidyr)
library(NHANES)
library(cowplot)
library(igraph)
library('MBESS')

```

First let's set up a simple linear model using simulated data.  Let's simulate the relationship between age and height.  We will select a set of ages from a uniform distribution between 6 and 18, and then generate heights and add random noise.  We will use parameter values estimated from the NHANES data, so let's first determine those.

```{r}

NHANES$isChild <- NHANES$Age<18
NHANES_child=subset(NHANES,subset=isChild & Height!='NA')
lm.result=lm(Height~Age,data=NHANES_child)
print(lm.result)

minAge=6
maxAge=18
nPoints=48
noiseLevel=sd(lm.result$residuals)
dataDf=data.frame(Age=minAge + (maxAge-minAge)*runif(nPoints))

# we can use the predict() function to predict heights for a new set of ages 
# based on the model estimated from the NHANES dataset

dataDf$Height=predict(lm.result,newdata=dataDf) + rnorm(nPoints,sd=noiseLevel)


```

We can compare the results of the same model applied to our simulated data, to make sure that they are similar.

```{r}
lm.resultSimulated=lm(Height~Age,data=dataDf)
print(summary(lm.resultSimulated))
```

Let's plot the data compared to the model and show the residuals.

```{r}
dataDf$predicted=predict(lm.resultSimulated)
dataDf$residual=lm.resultSimulated$residuals
p=ggplot(dataDf,aes(Age,Height)) +
  geom_point() + geom_smooth(method='lm',se=FALSE)
for (i in 1:dim(dataDf)[1]){
  p=p+annotate('segment',x=dataDf$Age[i],xend=dataDf$Age[i],
               y=dataDf$predicted[i],yend=dataDf$Height[i],color='red',linetype='dashed')
}
print(p)
```

We can plot the residuals in a couple of ways, to see what they look like.

First, let's just plot a histogram of residuals.

```{r}
ggplot(dataDf,aes(residual)) +
  geom_histogram()
```

They should be centered around zero and be roughly normally distributed.  We can also plot the residuals against age, to see if there is any relationship.

```{r}
ggplot(dataDf,aes(Age,residual)) +
  geom_point() + geom_smooth(method='lm',se=FALSE)

```

Here we don't see any relationship between age and the residuals - meaning that the model seems to fit equally well across the entire range of ages. This is confirmed by the fact that the line relating age and residuals falls squarely on top of the zero line.

#### Linearity

The data above were generated by a linear model.
Now let's look at what happens with the assumption of linearity is violated.
Let's pretend that we are looking at a new species of alien organisms, who grow for a few years but then start shrinking.  We can model this by making height a function of age squared, after centering age around zero; we multiply this by negative one to invert it.

```{r}
dataDf$AgeDemeaned=dataDf$Age-mean(dataDf$Age)
dataDf$AgeDemeanedSquaredInverted=-1*(dataDf$AgeDemeaned)**2

ggplot(dataDf,aes(Age,AgeDemeanedSquaredInverted )) +
         geom_point()

```

Now let's generate some data based on this relationship.

```{r}
dataDf$HeightNonlinear = lm.result$coefficients[1] + dataDf$AgeDemeanedSquaredInverted + rnorm(nPoints,sd=noiseLevel)

ggplot(dataDf,aes(Age,HeightNonlinear )) +
         geom_point()

```


Now you can see that there is pretty clearly a nonlinear relationship between age and height. We call this an "inverted U" relationship.  

Now let's say that we haven't looked very closely at our data, and we decided to fit a linear model to the data.  What happens?

```{r}
lm.resultSimulatedNonlinear=lm(HeightNonlinear~Age,data=dataDf)
print(summary(lm.resultSimulatedNonlinear))

```


Now we see that there is no significant relationship between age and height according to the model, even though there clearly is a relationship according to our eye!

Let's look at the residuals to see if this is evident.

```{r}
dataDf$residualNonlinear=lm.resultSimulatedNonlinear$residuals
ggplot(dataDf,aes(residualNonlinear)) +
  geom_histogram()

ggplot(dataDf,aes(Age,residualNonlinear)) +
  geom_point() + geom_smooth(method='lm',se=FALSE)

```

We don't really see anything obvious in the histogram, but it's clear in the plot of residuals versus age that there is somethign going on; even though the line still sits at zero, we can see that the residuals vary systematically with age.  

#### Additivity



#### Independence of errors

When we fit a model to data and compute p-values from the model, we assume that the errors are independent between the observations.  If we are sampling individuals randomly from the population then this should be true, but what happens if this assumption is violated?

let's pretend that food supply in our population fluctuates over time, such that people born near each other in time are more likely to have similar heights.


```{r}
correlatedNoise=as.vector(arima.sim(list(ar=0.9),n=nPoints))

dataDf$HeightCorr=predict(lm.result,newdata=dataDf) + correlatedNoise*10
# make a dataset with the same amount of noise, but independent
dataDf$HeightInd=predict(lm.result,newdata=dataDf) + rnorm(nPoints)*sd(correlatedNoise)

ggplot(dataDf,aes(Age,HeightCorr)) +
  geom_point()
```

Now let's use randomization testing to see whether this correlation has an effect on the p-values obtained from a simple linear model.

```{r}
nRuns=5000
pvalsCorr=array(NA,nRuns)
pvalsInd=array(NA,nRuns)
dataDfRand=dataDf
for (i in 1:nRuns){
  dataDfRand$Age=sample(dataDfRand$Age)
  modelRandInd=lm(HeightInd~Age,data=dataDfRand)
  pvalsInd[i]=summary(modelRandInd)$coefficients[2,4]
  modelRandCorr=lm(HeightCorr~Age,data=dataDfRand)
  pvalsCorr[i]=summary(modelRandCorr)$coefficients[2,4]
}
print(paste('Proportion of false positives (independent):',mean(pvalsInd<0.05)))
print(paste('Proportion of false positives (correlated):',mean(pvalsCorr<0.05)))

```

It appears that the correlation doesn't affect the Type I error of the test. What about its power?  To measure this, let's generate a bunch of datasets with either correlated or independent noise, and see what this does to the standard errors of the estimates and the power of the test.
```{r}

nRuns=5000
outputDf=data.frame(noiseType=array(NA,nRuns*2),
                    pval=array(NA,nRuns*2),
                    beta=array(NA,nRuns*2),
                    residualSE=array(NA,nRuns*2))

noiseLevel=20 # add a lot of noise here so that power is less than 1

for (i in seq(1,nRuns*2,2)){
  dataDfSim=data.frame(Age=dataDf$Age)
  correlatedNoise=as.vector(arima.sim(list(ar=0.9),n=nPoints))*noiseLevel
  correlatedNoise=correlatedNoise-mean(correlatedNoise)
  dataDfSim$HeightCorr=predict(lm.result,newdata=dataDfSim) + correlatedNoise
  dataDfSim$HeightInd=predict(lm.result,newdata=dataDfSim) + rnorm(nPoints)*sd(correlatedNoise)

  modelInd=lm(HeightInd~Age,data=dataDfSim)
  scInd=summary(modelInd)$coefficients
  outputDf[i,1]='Independent'
  outputDf[i,2:4]=c(scInd[2,4],scInd[2,1],sqrt(sum(modelInd$residuals**2))/sqrt(nPoints-2))

  modelCorr=lm(HeightCorr~Age,data=dataDfSim)
  scCorr=summary(modelCorr)$coefficients
  outputDf[i+1,1]='Correlated'
  outputDf[i+1,2:4]=c(scCorr[2,4],scCorr[2,1],sqrt(sum(modelCorr$residuals**2))/sqrt(nPoints-2))
}
```

```{r}
outputDf$noiseType=as.factor(outputDf$noiseType)
outputDf$sig=outputDf$pval<0.05
resultsDf=outputDf %>% group_by(noiseType) %>% 
    summarise(Mean.beta.estimate=mean(beta),
              SD.of.beta.estimates=sd(beta),
              Power=mean(sig),
              Mean.residual.SE=mean(residualSE))
print(resultsDf)

```

We see a couple of things here:

- The mean beta estimates for both datasets are very similar and very close to the true value (5.463).  This is consistent with our knowledge that correlated errors do not lead to biased parameter estimates. 
- The beta estimates are much more variable when the noise is correlated, compared to when the noise is independent.
- The power for the analyses with independent errors is substantially higher than the power of the anlayses with correlated errors. This occurs even though the residual error is actually a bit higher for the independent case.

Let's plot the distributions of parameter estimates for the two cases, to see just how they differ.
```{r}

a=ggplot(subset(outputDf,noiseType=='Independent'),aes(beta)) +
  geom_histogram(aes(y=..density..),bins=50) +xlim(-20,30) + ylim(0,0.3) +
  ggtitle('Independent') + xlab('Beta estimate')
b=ggplot(subset(outputDf,noiseType=='Correlated'),aes(beta)) +
  geom_histogram(aes(y=..density..),bins=50) +xlim(-20,30) + ylim(0,0.3) +
  ggtitle('Correlated')+ xlab('Beta estimate')
plot_grid(a,b,nrow=2)
```

### Homoscedasticity/homogeneity of variance

Another assumption that we make is that variances are the same for all observations.   This is particularly important when we are comparing two groups to one another.  For this example, let's simulate some data in which we look for the difference in height between children from two different groups: one with poor nutrition, and one with good nutrition.  Let's say that nutrition has an effect of 1/2 a standard deviation (which is a medium-sized effect).  

In this case, let's suppose that the variabilty is much larger in the poor nutrition group.

```{r}

nPerGroup=24
noiseSD=10
noiseSDdiff=2
groupDiffSD=0.5

groupDataDf=data.frame(Nutrition=c(rep('Good',nPerGroup),rep('Poor',nPerGroup)),
                       Height=c(rnorm(nPerGroup)*noiseSD+mean(NHANES_child$Height),rnorm(nPerGroup)*noiseSD*noiseSDdiff+mean(NHANES_child$Height)))
groupDataDf$Height[groupDataDf$Nutrition=='Good']=groupDataDf$Height[groupDataDf$Nutrition=='Good']+sd(groupDataDf$Height)*groupDiffSD
ggplot(groupDataDf,aes(y=Height,x=Nutrition)) +
  geom_boxplot()

t.test(Height~Nutrition,data=groupDataDf)
```

Let's first see what this unequal variances do to the type I error.  To test this, let's randomly generate data for two groups with equal means but unequal variances, and see how often we find a significant test. Because we know that the effects of unequal variance have the largest effect when the higher-variance group is also smaller, we simulate this situation.

```{r}
nRuns=5000
pvalsHetero=array(NA,nRuns)
pvalsEqual=array(NA,nRuns)
assumeEqualVariance=TRUE

groupSizes=c(72,24)
#groupSizes=c(48,48)

noiseSD=1
noiseSDdiff=4

for (i in 1:nRuns){
  groupDataDf=data.frame(Nutrition=c(rep('Good',groupSizes[1]),rep('Poor',groupSizes[2])),
                       HeightHetero=c(rnorm(groupSizes[1])*noiseSD+mean(NHANES_child$Height),
                                      rnorm(groupSizes[2])*noiseSD*noiseSDdiff+mean(NHANES_child$Height)),
                       HeightEqual=c(rnorm(sum(groupSizes))*noiseSD+mean(NHANES_child$Height)))
  pvalsEqual[i]=t.test(HeightEqual~Nutrition,data=groupDataDf,var.equal=assumeEqualVariance)$p.value
  pvalsHetero[i]=t.test(HeightHetero~Nutrition,data=groupDataDf,var.equal=assumeEqualVariance)$p.value
}
print(paste('Proportion of false positives (equal variance):',mean(pvalsEqual<0.05)))
print(paste('Proportion of false positives (unequal variance):',mean(pvalsHetero<0.05)))

```

Clearly, the standard t-test shows inflated Type I error in this situation.  This can be prevented by using an adaptation of the t-test that accounts for unequal variances. To see this, try seting assumeEqualVariance to FALSE and see how the result changes.

#### Normality and external variables

The methods that we use to compute p-values and confidence intervals generall require that the errors in the model are normally distributed.  Note that this doesn't mean that the data are normally distributed. Let's look at an example - let's say that the effect of nutrition on height was really large:

```{r}
groupSizes=c(128,128)
groupEffect=6
groupDataDf=data.frame(Nutrition=c(rep('Good',groupSizes[1]),rep('Poor',groupSizes[2])),
                       Height=rnorm(sum(groupSizes))+mean(NHANES_child$Height))

groupDataDf$Height[groupDataDf$Nutrition=='Good']=groupDataDf$Height[groupDataDf$Nutrition=='Good'] + groupEffect

ggplot(groupDataDf,aes(Height,fill=Nutrition)) +
  geom_histogram(bins=100)
```


If we were to build a model of the data that didn't include nutrition (which is what the book would refer to as an "external variable"), then the residuals will be highly non-normal:

```{r}
model0=lm(Height~1,data=groupDataDf)
ggplot(data.frame(residuals=model0$residuals),aes(residuals)) +
  geom_histogram(bins=50)
```

But if we build an appropriate model that accounts for the effect of nutrition, then the residuals should look roughly normal:

```{r}
model1=lm(Height~Nutrition,data=groupDataDf)
ggplot(data.frame(residuals=model1$residuals),aes(residuals)) +
  geom_histogram(bins=50)

```


In general, the statistical tests that we use are fairly robust to violations of normality, but this breaks down if the data are highly non-normal (as they will be in a long-tailed distribution) and/or the samples are very small.  

Let's run a simulation to see whether the t-test is robust to extreme non-normality.  Let's say that we want to compare the number of Facebook friends between students at Stanford and USC. Let's first see what happens in the case where there is no actual difference.

```{r}

nPerGroup=48
popSize=8000
popUSC=degree(barabasi.game(popSize))
popStanford=degree(barabasi.game(popSize))

groupDataDf=data.frame(Group=c(rep('Stanford',nPerGroup),rep('USC',nPerGroup)),
                       Nfriends=c(sample(popStanford,nPerGroup),
                                  sample(popUSC,nPerGroup)))



ggplot(groupDataDf,aes(x=Nfriends,color=Group)) +
  geom_freqpoly(binwidth=1) +
  xlim(min(groupDataDf$Nfriends),max(groupDataDf$Nfriends)+1)

t.test(Nfriends~Group,data=groupDataDf)

```


Let's simulate this situation to see how non-normality affects p-values. We will also use a test called the Wilcoxon rank sum test, which is a *nonparametric* test that does not assume normality. 

```{r}
nRuns=5000
pvalsBA=array(NA,nRuns)
pvalsBAlog=array(NA,nRuns)
pvalsBAw=array(NA,nRuns)
assumeEqualVariance=FALSE 

nPerGroup=16
popSize=8000
popUSC=degree(barabasi.game(popSize))
popStanford=degree(barabasi.game(popSize))

for (i in 1:nRuns){
# first create large populations that we will sample from
  groupDataDf=data.frame(Group=c(rep('Stanford',nPerGroup),rep('USC',nPerGroup)),
                       Nfriends=c(sample(popStanford,nPerGroup),
                                  sample(popUSC,nPerGroup)))
  groupDataDf$logNfriends=log(groupDataDf$Nfriends)
  pvalsBA[i]=t.test(Nfriends~Group,data=groupDataDf,var.equal=assumeEqualVariance)$p.value
  pvalsBAlog[i]=t.test(logNfriends~Group,data=groupDataDf,var.equal=assumeEqualVariance)$p.value
  pvalsBAw[i]=wilcox.test(Nfriends~Group,data=groupDataDf,exact=FALSE)$p.value

}
print(paste('Proportion of false positives (scale free network):',mean(pvalsBA<0.05)))
print(paste('Proportion of false positives (scale free network - log transform):',mean(pvalsBAlog<0.05)))
print(paste('Proportion of false positives (scale free network- Wilcoxon test):',mean(pvalsBAw<0.05)))

```

Here we see that the t-test applied to these non-normal data results in slightly conservative behavior.  Log-transforming the data seems to help a bit with this. 


Now let's see how well these different approaches work to detect a true effect when it exists --- that is, their power. This time, we will simulate data in which the Stanford students are roughly twice as friendly as the USC students, and see whether we can detect such a huge difference in a small sample of 24 subjects from each university.

```{r}
nRuns=5000
nPerGroup=24
popSize=8000
popUSC=degree(sample_pa(popSize))
popStanford=degree(sample_pa(popSize,m=2))

outputDf=data.frame(model=array(NA,nRuns*3),
                    pval=array(NA,nRuns*3))


for (i in seq(1,nRuns*3,3)){
  dataDf=data.frame(Group=c(rep('Stanford',nPerGroup),rep('USC',nPerGroup)),
                       Nfriends=c(sample(popStanford,nPerGroup),
                                  sample(popUSC,nPerGroup)))
  dataDf$logNfriends=log(dataDf$Nfriends)
  outputDf[i,1]='T-test(raw)'
  outputDf[i,2]=t.test(Nfriends~Group,data=dataDf,var.equal=FALSE)$p.value
  outputDf[i+1,1]='T-test(log)'
  outputDf[i+1,2]=t.test(logNfriends~Group,data=dataDf,var.equal=FALSE)$p.value
  outputDf[i+2,1]='Wilcoxon(raw)'
  outputDf[i+2,2]=wilcox.test(Nfriends~Group,data=dataDf,exact=FALSE)$p.value
}
```

```{r}
outputDf$model=as.factor(outputDf$model)
outputDf$sig=outputDf$pval<0.05
resultsDf=outputDf %>% group_by(model) %>% 
    summarise(Power=mean(sig)) 
print(resultsDf[order(resultsDf$Power,decreasing=TRUE),])

```

Here we see that the nonparametric test does the best, with the t-test on the log-transformed data close behind. The t-test on the raw data performs very badly, only detecting this effect about half as well as the Wilcoxon test.

#### Binary variables and dichotomization

One particular kind of non-normality happens when the outcome variable in the model is binary.  

Let's say that we are interested in understanding whether there is a relationship in the NHANES data between how many hours of TV a person watches per day and whether they are overweight (which is defined as a BMI greater than 25).  First let's plot the raw relationship between TV watching and BMI.

```{r}
NHANES$isChild <- NHANES$Age<18
NHANES_adult=subset(NHANES,subset=!isChild & BMI!='NA' & TVHrsDay!='NA')
NHANES_adult$TVHrsDayNum = dplyr::recode(as.character(NHANES_adult$TVHrsDay), 
                                      '0_hrs' = 0, '0_to_1_hr' = 0.5,
                                      '1_hr'=1,'2_hr'=2,'3_hr'=3,'4_hr'=4,'More_4_hr'=5)
# we need to recode the TV data into numbers
ggplot(NHANES_adult,aes(x=TVHrsDayNum,y=BMI)) +
  geom_point() + geom_smooth(method='lm')
print(summary(lm(BMI~TVHrsDayNum,data=NHANES_adult)))
```

In the full sample, there is a relationship such that every hour of TV watching per dat is associated with roughly 0.71 increase in BMI. But let's say that instead of having the BMI data, all we had was a binary measure of whether the individual was overweight or not.

```{r}
NHANES_adult$Overweight=NHANES_adult$BMI>25
print(summary(lm(Overweight~TVHrsDayNum,data=NHANES_adult)))


```

Here we see that every hour of TV watching was associated with an increase of .03 in the likelihood of being overweight.

Let's sample repeatedly and compare these two models. This is a fairly small effect so we need a pretty large sample size to find it at all - we will start with 250 subjects per sample.

```{r}
nRuns=5000
outputDf=data.frame(model=array(NA,nRuns*2),pval=array(NA,nRuns*2))
sampSize=250

randomize_x=FALSE

for (i in seq(1,nRuns*2,2)){
  NHANES_sample=sample_n(NHANES_adult,sampSize)
  if (randomize_x==TRUE){
    NHANES_sample$TVHrsDayNum=sample(NHANES_sample$TVHrsDayNum)
  }
  s1=summary(lm(BMI~TVHrsDayNum,data=NHANES_sample))
  outputDf[i,1]='BMI'
  outputDf[i,2]=s1$coefficients[2,4]
  
  s2=summary(lm(Overweight~TVHrsDayNum,data=NHANES_sample))
  outputDf[i+1,1]='Overweight'
  outputDf[i+1,2]=s2$coefficients[2,4]
  
}
```

```{r}
outputDf$model=as.factor(outputDf$model)
outputDf$sig=outputDf$pval<0.05
resultsDf=outputDf %>% group_by(model) %>% 
    summarise(Power=mean(sig)) 
print(resultsDf[order(resultsDf$Power,decreasing=TRUE),])

```

Here we see that our ability to detect the effect in the dichotomized data is substantially smaller than it is in the raw BMI data. This is perhaps not surprising, given that we threw away a lot of information when we dichotomized.  But it nonetheless serves as a good demonstration of the general point that dichotomizing data is almost never a good idea.

#### Multicollinearity

So far we have focused primarily on models where there is only one predictor variable, but it's often the case that there are multiple variables that may be relevant to some outcome.  When this happens, the results from our standard models can be highly misleading.

To demonstrate this most clearly, let's generate some simulated data.  Our outcome will be household income, and our two predictors will be IQ and parents' income.  We will use a function that generates a set of random normal variates with a given correlation.

```{r}
generateIncomeData = function(n,mu=c(100,100),sdev=c(10,10),
                              regcorr=0,betas=c(1,1),noiseSD=10){
  
  # we need to scale the betas since the standard deviations of the 
  # variables are very different
  cc=matrix(c(1,regcorr,regcorr,1),2,2)
  Sigma=cor2cov(cc,sdev)
  d=as.data.frame(mvrnorm(n = n,mu,Sigma))
  names(d)=c('IQ','ParentsIncome')
  # here we use matrix multiplication to generate the data
  d$Income=as.vector(mu[2] + as.matrix(d)%*%betas + rnorm(dim(d)[1])*noiseSD)
  return(d)
}
d=generateIncomeData(n=1000,regcorr=0.8)
cor(d)
a=ggplot(d,aes(x=IQ,y=Income))+geom_point()
b=ggplot(d,aes(x=ParentsIncome,y=Income))+geom_point()
plot_grid(a,b)
```

Now let's look what happens if we estimate the model using our standard approach. First, we estimate it in the case where IQ and Parent's Income are weakly correlated. Let's do this repeatedly and look at the power and the variability of the parameter estimates.

```{r}
nRuns=1000
outputDf=data.frame(param=array(NA,nRuns*4),
                    corr=array(NA,nRuns*4),
                    pval=array(NA,nRuns*4),
                    betaEst=array(NA,nRuns*4))

corrs=c(0.2,0.9)
betas=c(0.3,0.3)
for (i in seq(1,(nRuns-1)*4,4)){
  dfLowCorr=generateIncomeData(n=128,regcorr=corrs[1],betas=betas)
  s=summary(lm(Income ~ IQ + ParentsIncome,data=dfLowCorr))
  outputDf[i,1]='IQ'
  outputDf[i+1,1]='ParentsIncome'
  outputDf[i:(i+1),2]=corrs[1]
  outputDf[i,3:4]=c(s$coefficients[2,4],s$coefficients[2,1])
  outputDf[i+1,3:4]=c(s$coefficients[3,4],s$coefficients[3,1])
  dfHiCorr=generateIncomeData(n=128,regcorr=corrs[2],betas=betas)
  s=summary(lm(Income ~ IQ + ParentsIncome,data=dfHiCorr))
  outputDf[i+2,1]='IQ'
  outputDf[i+3,1]='ParentsIncome'
  outputDf[(i+2):(i+3),2]=corrs[2]
   outputDf[i+2,3:4]=c(s$coefficients[2,4],s$coefficients[2,1])
  outputDf[i+3,3:4]=c(s$coefficients[3,4],s$coefficients[3,1])
}

outputDf$sig=outputDf$pval<0.05
resultsDf=outputDf %>% group_by(param,corr) %>% 
    summarise(Power=mean(sig),MeanBeta=mean(betaEst),SdBeta=sd(betaEst)) 
print(resultsDf)

```

Let's plot a histogram of the parameter estimates across runs.

```{r}
ggplot(subset(outputDf,param='IQ') %>% drop_na(),aes(betaEst,color=as.factor(corr))) + geom_freqpoly(bins=50,size=1) +
  xlab('IQ parameter estimates')
```

